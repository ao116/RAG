{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fb4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us during the day because of a phenomenon called scattering. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter the shorter (blue) wavelengths of light more than the longer (red) wavelengths. This scattering effect is known as Rayleigh scattering.\n",
      "\n",
      "Here's why this works:\n",
      "\n",
      "1.  **Wavelengths of Light**: Sunlight contains a spectrum of colors, ranging from red to violet. Blue light has a relatively short wavelength compared to other colors.\n",
      "2.  **Scattering in the Atmosphere**: When sunlight enters the atmosphere, it encounters many tiny molecules. These molecules scatter shorter wavelengths more than longer wavelengths.\n",
      "3.  **What is Scattered Back?** As blue light is scattered, much of it is directed back towards our eyes from all parts of the sky. This blue light is visible to us as the blue color of the sky.\n",
      "\n",
      "This phenomenon happens on a daily basis and is responsible for making the sky appear blue.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# or access fields directly from the response object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m.content)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "# from ollama import chat\n",
    "# from ollama import ChatResponse\n",
    "\n",
    "# response: ChatResponse = chat(model='llama3.1:latest', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# print(response['message']['content'])\n",
    "# # or access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# client = ollama.Client(host=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf0ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"llama3.1:latest\",\"model\":\"llama3.1:latest\",\"modified_at\":\"2024-09-24T16:07:40.145280835+03:30\",\"size\":4661230766,\"digest\":\"42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_0\"}}]}"
     ]
    }
   ],
   "source": [
    "# !curl http://localhost:11434/api/tags\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bf30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello from Jupyter, running happily on Windows Subsystem for Linux (WSL)!\"\n"
     ]
    }
   ],
   "source": [
    "# import ollama\n",
    "# client = ollama.Client(host=\"http://localhost:11434\")\n",
    "\n",
    "# resp = client.generate(model=\"llama3.1:latest\",\n",
    "#                        prompt=\"Say hi from Jupyter on WSL in one line.\")\n",
    "# print(resp[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea2179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three bullet productivity tips:\n",
      "\n",
      "• **Prioritize tasks using the Eisenhower Matrix**: Divide your tasks into four quadrants based on their urgency and importance:\n",
      "\t+ Urgent & Important (Do First)\n",
      "\t+ Not Urgent but Important (Schedule)\n",
      "\t+ Urgent but Not Important (Delegate)\n",
      "\t+ Not Urgent or Important (Delete)\n",
      "\n",
      "• **Use a \"Stop Doing\" list to boost productivity**: Identify tasks, habits, or commitments that are no longer serving you and eliminate them. This can help free up time and energy for more important things.\n",
      "\n",
      "• **Implement the \"2-Minute Rule\"**: If a task can be done in less than 2 minutes, do it immediately. This can help prevent procrastination and get small tasks out of the way so you can focus on bigger projects."
     ]
    }
   ],
   "source": [
    "# for chunk in client.generate(model=\"llama3.1:latest\",\n",
    "#                              prompt=\"Give 3 bullet productivity tips.\",\n",
    "#                              stream=True):\n",
    "#     print(chunk[\"response\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353c7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b964fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE = \"http://localhost:11434\"\n",
    "\n",
    "def llm_generate(prompt: str, model: str = \"llama3.1:latest\") -> str:\n",
    "    url = f\"{BASE}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,   # keep it simple first\n",
    "    }\n",
    "    output = requests.post(url, json=payload, stream=False)\n",
    "    output.raise_for_status()\n",
    "    return output.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f07e1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How's it going? Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed559230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(text:str, model='nomic-embed-text:latest') -> np.ndarray:\n",
    "    url = f\"{BASE}/api/embed\"\n",
    "    params = {\n",
    "        'model' : model,\n",
    "        'input': text\n",
    "    }\n",
    "    get = requests.post(url, json=params)\n",
    "    get.raise_for_status()\n",
    "    return get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf44043",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = embedding('hello world it was my dream to be here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ecf9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check.json()['embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6354023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.array([1,2,3])\n",
    "check2 = np.array([2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76af431a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check @ check2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1:np.array, vec2:np.array) -> float:\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "choonezan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
